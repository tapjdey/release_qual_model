# creating a data.frame of all predictions for all customers for all products.
for(i in 1:length(all.predictions)){
cust.names <- names(all.predictions)[i]
ex.df <- as.data.frame(all.predictions[i])
try(colnames(ex.df) <- c("product","score"))
try(pos <- which(names(test.y) == cust.names))
try(new.items <- as.vector(unlist(test.y[pos])))
try(all.preds <- rep(0,length(products)))
try(all.preds[which(products %in% ex.df$product)] <- ex.df$score)
try(customer <- rep(cust.names,nrow(test.results)))
try(test.results <- data.frame(customer,prod=products,prediction=all.preds))
try(results.df <- rbind(results.df,test.results))
print(paste0("Organizing Predictions for Customer: ",i," / ",length(all.predictions)))
}
return(results.df)
}
predictions <- predict.fx("2008-08-31")
rm(list=ls())
# FUNCTION TO CREATE BASETABLE
create_BT <- function(t1, t2, t3, t4,tr_data){
# INSTALL PACKAGES
if (!require("plyr")) install.packages('plyr'); library('plyr')
# LOADS DATA
hh_demo = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/hh_demographic.csv")
coup.red = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/coupon_redempt.csv")
#filtering customers
cust.pur = ddply(tr_data,.(household_key),summarise,
firstday = min(DAY), lastday = max(DAY))
cust.pur.filtered = cust.pur[which(cust.pur$firstday<t2 & cust.pur$lastday> t2-365),]
# Since household data is for only a few, adding a new level -> no data
c1 = merge(hh_demo,cust.pur.filtered, all = T)
fcts = sapply(c1, is.factor)
c1.1 = sapply(c1[,fcts], as.character)
c1.1[is.na(c1.1)] = "NO DATA"
c1.1 = sapply(c1.1, as.factor)
c1[,fcts] = factor(c1.1)
c1[sapply(c1, is.character)] <- lapply(c1[sapply(c1, is.character)], as.factor)
# number of coupons redeemed in independent period
coup.red = coup.red[which(coup.red$DAY<t2),]
coup.agg = ddply(coup.red, .(household_key), summarise, coup.no = length(COUPON_UPC))
c2 = merge(c1, coup.agg, all = T)
c2$coup.no[is.na(c2$coup.no)] = 0
tr_data.filtered = tr_data[which(tr_data$DAY<t2),]
tr_data.agg = ddply(tr_data.filtered,.(household_key),summarise,
no.basket = length(unique(BASKET_ID)),
no.product = length(unique(PRODUCT_ID)),
sum.quant = sum(QUANTITY),
tot.sales.value = sum(SALES_VALUE),
stores = length(unique(STORE_ID)),
retail.disc = sum(RETAIL_DISC),
coup.disc = sum(COUPON_DISC))
base = merge(tr_data.agg,c2)
# RETURN BASETABLE
return (base)
}
# FUNCTION TO BUILD MODE
model.build <- function(t1,t2,t3,t4,tr_data){
# CALL BASETABLE FUNCTION
base <- create_BT(t1, t2, t3, t4,tr_data)
length_ind <- t2 - t1
length_op <- t3 - t2
length_dep <- t4 - t3
## COMPUTE DV (1 if churn, else 0)
base$DV = as.factor(ifelse(base$lastday > t4,0,1))
# INSTALL PACKAGES
if (!require("randomForest")) {
install.packages('randomForest',
repos="https://cran.rstudio.com/",
quiet=TRUE)
require('randomForest')
}
if (!require("AUC")) {
install.packages('AUC',
repos="https://cran.rstudio.com/",
quiet=TRUE)
require('AUC')
}
if (!require("lift")) {
install.packages('lift',
repos="https://cran.rstudio.com/",
quiet=TRUE)
require('lift')
}
# RANDOMIZE INDICATORS
ind <- 1:nrow(base)
indTRAIN <- sample(ind,round(0.5*length(ind)))
indTEST <- ind[-indTRAIN]
# ISOLATE DV
DV <-base$DV
# DELETE COLUMNS
base$DV <- NULL
base$lastday <- NULL
# RUN MODEL
rFmodel <- randomForest(x=(base[indTRAIN,]),
y=DV[indTRAIN],
ntree=1000)
predrF <- predict(rFmodel,base[indTEST,],type="prob")[,2]
cat("AUC under ROC curve of the model:", AUC::auc(roc(predrF,DV[indTEST])))
cat("\nTop Decile Lift:", TopDecileLift(predrF,DV[indTEST]))
varImpPlot(rFmodel)
# RETURN MODEL, LENGTH OF INDEPENDENT, OPERATIONAL, DEPENDENT
return (list(rFmodel, length_ind, length_op, length_dep,predrF,DV[indTEST]))
}
# FUNCTION TO DEPLOY MODEL
model.predict <- function(object, dumpDate){
# TIMELINE
t2 <- dumpDate
t1 <- t2 - object[[2]]
t3 <- t2 + object[[3]]
t4 <- t3 + object[[4]]
# SAVE MODEL
rFmodel <- object[[1]]
# CREATE BASETABLE
predbase <- create_BT(t1, t2, t3, t4,tr_data)
predbase$lastday <- NULL
# RUN PREDICTIONS
predrF <- predict(rFmodel, predbase, type="prob")[,2]
# STORE OUTPUT (CUSTOMERID AND SCORE)
ans <- data.frame("Customer_Id" = predbase$household_key, "Score" = predrF)
ans <- ans[order(ans$Score, decreasing=TRUE),]
# RETURN ANSWER
ans
}
tr_data = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/transaction_data.csv")
tr_data = tr_data[which(tr_data$QUANTITY != 0 & tr_data$SALES_VALUE != 0),]
t1 = min(tr_data$DAY)
t4 = max(tr_data$DAY) - 90
t3 = t4 - 180
t2 = t3 - 1
# CALL MODEL.BUILD
Cmodel <- model.build(t1, t2, t3, t4,tr_data)
# CALL MODEL.PREDICT
pred <- model.predict(object=Cmodel, dumpDate=max(tr_data$DAY))
p = Cmodel[[5]]
c = Cmodel[[6]]
library(ROCR)
pr = prediction(p,c)
perf <- performance(pr, measure="prec", x.measure="cutoff")
plot(perf)
perf <- performance(pr, measure="rec", x.measure="cutoff")
plot(perf)
library(DMwR)
PRcurve(p,c)
#From the curves, it seems a threshold of around 0.35 is reasonable
table(p<0.35,c)
churn.hou = pred[which(pred$Score>0.35),]$Customer_Id
length(churn.hou)
library(recommenderlab)
library(ggplot2)
library(reshape2)
hh.tr.data = ddply(tr_data, .(PRODUCT_ID), summarize,tot.quantity = sum(QUANTITY))
prod.consider = unique(hh.tr.data[which(hh.tr.data$tot.quantity>20),]$PRODUCT_ID)
tr.data.filtered = tr_data[which(tr_data$PRODUCT_ID %in% prod.consider),]
rm(tr_data)
rm(hh.tr.data)
hh.tr.data = ddply(tr.data.filtered, .(household_key,PRODUCT_ID), summarize,tot.quantity =log(length(QUANTITY)))
g = acast(hh.tr.data, household_key~PRODUCT_ID)
r <- as(g, "realRatingMatrix")
#Evaluation of different schemes
qplot(getRatings(r), binwidth = 0.5,
main = "Histogram of log of quantities", xlab = "log Qty") # Skewed to the left
qplot(getRatings(normalize(r, method = "Z-score")),
main = "Histogram of log of quantities", xlab = "log Qty") # better
#since log(2) = 0.69, we're saying goodRating = 0.7, meaning they will buy it 2 times
scheme <- evaluationScheme(r, method = "cross-validation", train = .9,
k = 10, given = -1, goodRating = 0.7)
scheme
# Not looking at IBCF, since that won't help our cause here
algorithms <- list(
"random items"  = list(name="RANDOM", param=list(normalize = "Z-score")),
"popular items" = list(name="POPULAR", param=list(normalize = "Z-score")),
"user-based CF" = list(name="UBCF", param=list(normalize = "Z-score",
method="Cosine",
nn=20))
)
# run algorithms, predict next n items
results <- evaluate(scheme, algorithms, n=c(1, 3, 5, 10, 15))
# Draw ROC curve
plot(results, annotate = 1:4, legend="topleft")
# See precision / recall
plot(results, "prec/rec", annotate=3)
?cite
Rec.model=Recommender(r,method="UBCF",
param=list(normalize = "Z-score",method="Cosine",nn=20))
pr = predict(Rec.model, r[churn.hou,], n=3)
r
qplot(rowCounts(r), binwidth = 10,
main = "Products bought on average",
xlab = "# of users",
ylab = "# of products purchased")
as(pr, "list")
prod = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/product.csv")
View(prod)
cite(Recommender())
?cite
cite("R")
citation()
citation("RandomForest")
citation("randomForest")
citation("lift")
citation("AUC")
citation("plyr")
citation("recommenderlab")
system("python Desktop/s.py")
?rnorm
setwd("~/Work/release_qual/model/images")
#library
library(plyr)
library(GGally)
library(ggplot2)
library(psych)
library(DataCombine)
library(reshape)
library(gridExtra)
library(grid)
#Reading .user data
filelist <- list.files(path = "../../data/data1_May16/", pattern = "*4.new.user", full.names = TRUE)
#Reading New data
filelist1 <- list.files(path = "../../data/data1_May16/", pattern = "*4.new$", full.names = TRUE)
UserFile = do.call(rbind, lapply(filelist, function(x) read.csv(file = x,sep=";",na.strings="(not set)")))
UserFile[,7] <- as.Date(UserFile[,7],"%Y%m%d")
UserFile[,8] <- as.integer(UserFile[,8])
UserFile[,9] <- as.integer(UserFile[,9])
UserFile[,10] <- as.numeric(UserFile[,10])
UserFile <- UserFile[order(UserFile[,7]),]
NewData <- do.call(rbind, lapply(filelist1, function(x) read.csv(file = x,sep=";")))
NewData[,3] <- as.Date(as.character(NewData[,3]),"%Y%m%d")
NewData <- NewData[order(NewData[,3]),]
total = merge(NewData,UserFile,all=T)
total = total[complete.cases(total),]
total$ga.deviceCategory <- factor(total$ga.deviceCategory)
total$ga.fatalExceptions <- NULL
total$ga.timeOnSite  <- NULL
###############################################################################################################
###############################################################################################################
library(plyr)
library(GGally)
library(ggplot2)
library(psych)
#2.0.0_317 last day,2.0.0_326 first 2 days,2.0.0_350 last day, 435 last day, 2.1.0_483 last 2, 503 first, 3.0.0_197 last
total = total[!rownames(total) %in% c(504,759,761,2010,12248,13245,13244,13825,49530,48963),]
#collapsing all releases to get uniform curve
releases = unique(total$ga.appVersion)
goodrelease = c()
rd = matrix(nrow = 0, ncol=10)
for (r in sort(releases)){
y= total[total$ga.appVersion == r,]
z= ddply(y, .(ga.date), summarise,
nu = sum(ga.newUsers),
nv = sum(ga.newVisits),
tu = sum(ga.users),
tv = sum(ga.visits),
ex = sum(ga.exceptions))
#removing suspicious releases
if (quantile(z$nu,0.1) == quantile(z$nu,0.9) || quantile(z$nv,0.1) == quantile(z$nv,0.9)) next()
#keeping only releases with >7 days of data or non zero number of exceptions
if(nrow(z) > 7){
z$cnu = cumsum(z$nu)
quantile(z$cnu)
goodrelease = c(goodrelease,r)
png(paste0(r,".png"),width = 960, height = 960)
pairs.panels(z)
dev.off()
rd = rbind(rd,(c(r, min(z$ga.date), max(z$ga.date),(z[which(z$cnu > quantile(z$cnu,0.5)),]$ga.date)[1],
(z[which(z$cnu > quantile(z$cnu,0.25)),]$ga.date)[1],(z[which(z$cnu > quantile(z$cnu,0.75)),]$ga.date)[1],
(z[which(z$cnu >= quantile(z$cnu,0.9)),]$ga.date)[1],sum(z$nu),sum(z$ex),sum(z$nv))))
}
}
rd = as.data.frame(rd)
colnames(rd) = c("Release","Mindate","Maxdate","Median","25.Quant","75.Quant","90.Quant","Total.NU","ex","Total.NV")
rd$Total.NU = as.numeric(as.character(rd$Total.NU))
Total.NU = as.numeric(as.character(rd$Total.NU))
rd$Total.NV = as.numeric(as.character(rd$Total.NV))
ex = as.numeric(as.character(rd$ex))
rd$ex = ex
rd$Mindate = as.Date(as.integer(as.character(rd$Mindate)),origin="1970-01-01")
rd$Maxdate = as.Date(as.integer(as.character(rd$Maxdate)),origin="1970-01-01")
rd$Median = as.Date(as.integer(as.character(rd$Median)),origin="1970-01-01")
rd$`25.Quant` = as.Date(as.integer(as.character(rd$`25.Quant`)),origin="1970-01-01")
rd$`75.Quant` = as.Date(as.integer(as.character(rd$`75.Quant`)),origin="1970-01-01")
rd$`90.Quant` = as.Date(as.integer(as.character(rd$`90.Quant`)),origin="1970-01-01")
rd$Release = as.character(rd$Release)
refr = rd
tick.col = c()
for (i in 1:nrow(rd)){
if (ecdf(Total.NU)(rd[i,8]) <0.2){
rd[i,1] = paste0("",rd[i,1])
} else if (ecdf(Total.NU)(rd[i,8]) <0.5){
rd[i,1] = paste0("*",rd[i,1])
}else if (ecdf(Total.NU)(rd[i,8]) <0.8){
rd[i,1] = paste0("***",rd[i,1])
}else {
rd[i,1] = paste0("******",rd[i,1])
}
if (rd[i,9] == 0) {
tick.col = c(tick.col,"green")
} else if (rd[i,9] < 50){
tick.col = c(tick.col,"black")
} else if (rd[i,9] < 300){
tick.col = c(tick.col,"blue")
} else if (rd[i,9] < 750){
tick.col = c(tick.col,"orange")
}else {
tick.col = c(tick.col,"red")
}
}
rd$`25.Quant` = NULL
rd$`75.Quant` = NULL
rd$ex = NULL
rd$Total.NU = NULL
rd$Total.NV = NULL
# release dates added manually
manrd = data.frame("Release"=as.factor(c("******2.1.2_568","***2.1.2_574","******2.1.4_577","******2.0.0_340","******2.0.1_372","******2.1.0_527")),
"Release Date"=as.Date(c("2015-10-02","2016-01-06","2016-01-22","2014-03-17","2014-05-20","2014-12-15")))
nrd = merge(rd,manrd,all = T)
r=melt(nrd, id="Release")
ggplot(r) + geom_point(aes(x=Release, y=value, colour=variable)) +
scale_color_manual(values = c("red","blue","black","magenta","cyan"))+ ylab("Year")+  theme_bw() +
theme(axis.text.y = element_text(colour=tick.col), axis.text.x = element_text(colour="black")) +
coord_flip()
##########################################################
library(bnlearn)
refr$Release = as.factor(refr$Release)
refr$duration = as.numeric(refr$`90.Quant` - refr$Mindate)
refr$f.duration = as.numeric(refr$Maxdate - refr$Mindate)
refr[,3:7] = NULL
refr$Release = NULL
refr$VperU = refr$Total.NV / refr$Total.NU
refr$durFromLast = rep(0, nrow(refr))
for (i in 2:nrow(refr)){
refr[i,8] = refr[i,1] - refr[(i-1),1]
}
refr2 = refr
refr2$Mindate = as.numeric(refr2$Mindate)
refr = refr2
tnv = refr$Total.NV
refr$Total.NV = NULL
refr$f.duration = NULL
refr$Mindate = NULL
pdag1 = iamb(refr)
plot(pdag1)
dag1 = cextend(pdag1)
plot(dag1)
b1 = bn.fit(dag1,refr)
print(b1$Total.NU)
dag2 = tabu(refr)
plot(dag2)
(b2 = bn.fit(dag2, refr))
dag3 = rsmax2(refr, restrict = "si.hiton.pc", maximize = "tabu",
test = "zf", alpha = 0.1, score = "bic-g")
plot(dag3)
b3 = bn.fit(dag3, refr)
# discrete?
drefr = discretize(refr, method = "hartemink", breaks = 3, ibreaks = 4, idisc = "quantile")
# deal
library(deal)
deal.net = network(drefr)
prior = jointprior(deal.net)
deal.net = learn(deal.net, drefr, prior)$nw
deal.best = autosearch(deal.net, drefr, prior)
bnlearn:::fcat(deal::modelstring(deal.best$nw))
#catnet
library(catnet)
netlist = vector(200, mode = "list")
ndata = nrow(drefr)
nodes = names(drefr)
netlist = lapply(netlist, function(net) {
boot = drefr[sample(ndata, replace = TRUE), ]
nets = cnSearchOrder(boot)
best = cnFindBIC(nets, ndata)
cnMatEdges(best)
})
sa = custom.strength(netlist, nodes = nodes)
avg.catnet = averaged.network(sa, threshold = 0.7)
avg.catnet
plot(avg.catnet)
#boot
(boot = boot.strength(data = drefr, R = 200, algorithm = "hc",
algorithm.args = list(score = "bde", iss = 10)))
avg.boot = averaged.network(boot, threshold = 0.85)
plot(avg.boot)
#pcalg - doesn't work
# library(pcalg)
# suffStat = list(dm = sapply(drefr,as.integer)-1, nlev = sapply(drefr, nlevels),adaptDF = FALSE)
# pcalg.net = pc(suffStat, indepTest = disCItest, p = ncol(drefr),alpha = 0.05)
#mbde
refr$Total.NU = log(refr$Total.NU+1)
drefr = discretize(refr, method = "hartemink", breaks = 3, ibreaks = 4, idisc = "quantile")
nodes = names(drefr)
start = random.graph(nodes = nodes, method = "melancon", num = 500,
burn.in = 10^5, every = 100)
netlist = lapply(start, function(net) {tabu(drefr, score = "bde",
iss = 1, start = net, tabu = 50) })
arcs = custom.strength(netlist, nodes = nodes, cpdag = F)
plot(arcs)
bn.mbde = averaged.network(arcs, threshold = 0.5)
plot(bn.mbde)
(b = bn.fit(bn.mbde, refr))
plot(b2)
plot(dag2)
b2
?base::"abs"
?base:"abs"
?::
?base:abs
?base::abs
b2$Total.NU$sd
b2$Total.NU$residuals
b2$Total.NU$node
b2$Total.NU$parents
b2$Total.NU$children
b2$Total.NU$fitted.values
2^5
5**2
b2$Total.NU$sd**2 + b2$ex$sd**2 + b2$VperU$sd**2
plot(dag3)
b3$Total.NU$sd**2 + b3$ex$sd**2 + b3$VperU$sd**2
?cut
plot(avg.catnet)
plot(sa)
catfit = bn.fit(avg.catnet,drefr)
plot(bn.mbde)
(b = bn.fit(bn.mbde, refr))
bn.mbde$nodes$Total.NU$mb
bn.mbde$nodes$Total.NU$nbr
b$Total.NU$sd**2 + b$VperU$sd**2
?proc.time
?time
?system.time
?paste
custom = paste("[duration][ex][durFromLast][Total.NU|duration:durFromLast:ex][VperU|Total.NU]")
custom.net = model2network(custom)
library(Rgraphviz)
graphviz.plot(custom.net, shape = "ellipse")
(b = bn.fit(bn.mbde, refr))
b$Total.NU$sd**2 + b$VperU$sd**2
b2$Total.NU$sd**2 + b2$ex$sd**2 + b2$VperU$sd**2
b2$Total.NU$sd
b2$ex$sd
b2$VperU$sd
refr$Total.NU = log(refr$Total.NU+1)
dag2 = tabu(refr)
plot(dag2)
(b2 = bn.fit(dag2, refr))
View(refr)
hist(refr$duration)
refr$duration = log(refr$duration+1)
View(refr)
refr$ex = log(refr$ex + 1)
refr$VperU = log(refr$VperU + 1)
refr$durFromLast = log(abs(refr$durFromLast)+1)
View(refr)
refr$VperU = exp(refr$VperU)+1
refr = refr2
View(refr)
refr$Total.NU = log(refr$Total.NU+1)
refr$duration = log(refr$duration+1)
refr$ex = log(refr$ex + 1)
refr$VperU = log(refr$VperU)
refr$durFromLast = log(abs(refr$durFromLast)+1)
refr$Total.NV = NULL
refr$f.duration = NULL
refr$Mindate = NULL
pdag1 = iamb(refr)
plot(pdag1)
dag1 = cextend(pdag1)
plot(dag1)
b1 = bn.fit(dag1,refr)
print(b1$Total.NU)
dag2 = tabu(refr)
plot(dag2)
(b2 = bn.fit(dag2, refr))
dag2 = tabu(refr)
plot(dag2)
(b2 = bn.fit(dag2, refr))
b2$ex$sd**2 + b2$duration$sd**2 + b2$VperU$sd**2
drefr = discretize(refr, method = "hartemink", breaks = 3, ibreaks = 4, idisc = "quantile")
nodes = names(drefr)
start = random.graph(nodes = nodes, method = "melancon", num = 500,
burn.in = 10^5, every = 100)
netlist = lapply(start, function(net) {tabu(drefr, score = "bde",
iss = 1, start = net, tabu = 50) })
arcs = custom.strength(netlist, nodes = nodes, cpdag = F)
plot(arcs)
bn.mbde = averaged.network(arcs, threshold = 0.5)
plot(bn.mbde)
(b = bn.fit(bn.mbde, refr))
b$Total.NU$sd**2 + b$ex$sd**2 + b$VperU$sd**2
custom = paste("[duration][ex][durFromLast][Total.NU|duration:durFromLast:ex][VperU|Total.NU]")
custom.net = model2network(custom)
library(Rgraphviz)
graphviz.plot(custom.net, shape = "ellipse")
(b = bn.fit(bn.mbde, refr))
b$Total.NU$sd**2 + b$VperU$sd**2
b2$ex$sd**2 + b2$duration$sd**2 + b2$VperU$sd**2 + b2$Total.NU$sd**2 + b2$durFromLast$sd**2
b$ex$sd**2 + b$duration$sd**2 + b$VperU$sd**2 + b$Total.NU$sd**2 + b$durFromLast$sd**2
nodes = names(drefr)
start = random.graph(nodes = nodes, method = "melancon", num = 500,
burn.in = 10^5, every = 100)
netlist = lapply(start, function(net) {tabu(drefr, score = "bde",
iss = 1, start = net, tabu = 50) })
arcs = custom.strength(netlist, nodes = nodes, cpdag = F)
plot(arcs)
bn.mbde = averaged.network(arcs, threshold = 0.5)
plot(bn.mbde)
(b = bn.fit(bn.mbde, refr))
b$ex$sd**2 + b$duration$sd**2 + b$VperU$sd**2 + b$Total.NU$sd**2 + b$durFromLast$sd**2
?boxcox
library(sos)
findFn("boxcox")
library(MASS)
?boxcox
