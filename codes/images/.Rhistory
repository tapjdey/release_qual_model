visits$VisitDate <- as.Date(visits$VisitDate,format="%d-%b-%y")
visits$year <- year(visits$DateTime)
visits$month <- month(visits$DateTime)
visits$mday <- day(visits$DateTime)
visits$hour <- hour(visits$DateTime)
customers$CustomerID <- as.character(customers$CustomerID)
####################################################################################
################
### Cleaning ###
################
prods <- prods[-which(prods$Family == ""),]
deets <- deets[which(deets$Quantity >= 0),]
prods <- prods[which(prods$Price >= 0),]
visits <- visits[which(visits$Amount >=0),]
####################
### Table Merges ###
####################
cust.routes <- merge(customers,routes,by="RouteID",all.x=TRUE)
cust.routes.visits <- merge(cust.routes,visits,by="CustomerID",all.x = TRUE)
deets.prods <- merge(deets,prods,by="ProductID")
base.table <- merge(cust.routes.visits,deets.prods,by="VisitID",all.x=TRUE)
################
# More Cleaning
################
sapply(base.table,function(x){length(which(is.na(x)))})
base.table <- base.table[-which(is.na(base.table$VisitID)),]
# These time periods are almost arbitrary. I fully expect them too be changed.
start.time <- Sys.time()
base.table <- base.table[-which(is.na(base.table$ProductID)),]
# Setting all dates for train set and test set
test.t4 <- your.t3.date + 90
test.t3 <- your.t3.date
test.t2 <- test.t3-1
test.t1 <- test.t2 - 150
train.t2 <- test.t1
train.t1 <- train.t2 - 150
train.t3 <- train.t2 + 1
train.t4 <- train.t3 + 90
# Collecting all purchases that occurred in test independent period
test.prev.purchases <- base.table[which(base.table$VisitDate <= test.t2 &
base.table$VisitDate >= test.t1 &
base.table$Outcome == "SALES"),] ; head(test.prev.purchases)
# Collecting all purchases taht occurred in train independent period
train.prev.purchases <- base.table[which(base.table$VisitDate <= train.t2 &
base.table$VisitDate >= train.t1 &
base.table$Outcome == "SALES"),] ; head(train.prev.purchases)
# Names of Customers that made purchases before t2
test.customers <- unique(test.prev.purchases$CustomerID)
train.customers <- unique(train.prev.purchases$CustomerID)
# Collecting all purchase information that occurred in dependent period.
test.post.purchases <- base.table[which(base.table$VisitDate >= test.t3 &
base.table$VisitDate <= test.t4 &
base.table$CustomerID %in% test.customers &
base.table$Outcome == "SALES"),]
train.post.purchases <- base.table[which(base.table$VisitDate >= train.t3 &
base.table$VisitDate <= train.t4 &
base.table$CustomerID %in% train.customers &
base.table$Outcome == "SALES"),]
# Writing functions to build lists of products for independent and dependent periods for customers
train.x.lists <- function(x){
return(unique(train.prev.purchases$ProductID[which(train.prev.purchases$CustomerID == x)]))
}
train.y.lists <- function(x){
return(unique(train.post.purchases$ProductID[which(train.post.purchases$CustomerID == x)]))
}
test.x.lists <- function(x){
return(unique(test.prev.purchases$ProductID[which(test.prev.purchases$CustomerID == x)]))
}
test.y.lists <- function(x){
return(unique(test.post.purchases$ProductID[which(test.post.purchases$CustomerID == x)]))
}
# Deploying above functions
train.customers <- as.character(unique(train.prev.purchases$CustomerID))
test.customers <- as.character(unique(test.prev.purchases$CustomerID))
train.x <- lapply(sapply(train.customers,train.x.lists),as.character)
train.y <- lapply(sapply(train.customers,train.y.lists),as.character)
test.x <- lapply(sapply(test.customers,test.x.lists),as.character)
test.y <- lapply(sapply(test.customers,test.y.lists),as.character)
#Initializing an empty data frame to be filled in by loop
sim.df <- data.frame(test_customer=NULL,train_customer=NULL,similarity=NULL,shared_prods = NULL,num_prods=NULL)
num.prods <- unlist(lapply(train.x,function(x){return(length(unique(unlist(x))))}))
# Looping through each customer in the test set to find nearest neighbors.
# This takes my computer about 2 minutes to run.
for(i in 1:length(test.x)){
cust.name <- names(test.x)[i]
items.bought <- as.vector(unlist(test.x[i]))
num.items <- length(items.bought)
head(train.x)
# Only comparing customers that have same number of unique items (+ or - 1 item)
cust2.list <- train.x[which(num.prods > (0.8*num.items) & num.prods < (1.2*num.items))]
#cust2.list <- train.x[which(names(train.x) %in% same.region)]
cust2s <- names(cust2.list)
all.sims <- lapply(cust2.list,function(x){return(length(intersect(items.bought,as.vector(unlist(x)))))})
shared.prods <- as.vector(unlist(all.sims))
similarities <- as.vector(unlist(all.sims) / num.items)
try(new.rows <- data.frame(test_customer=cust.name,train_customer = cust2s, similarity=similarities,shared_prods=shared.prods,num_prods=num.items))
try(new.rows <- new.rows[order(new.rows$similarity,decreasing = TRUE),])
k <- 55 #only taking top k most similar machines. This number can be changed.
try(new.rows <- new.rows[c(1:k),])
try(row.names(new.rows) <-  NULL) #row names will be name of machines otherwise
try(sim.df <- rbind(sim.df,new.rows)) # adding the new rows to the initialized data frame
print(paste0("Finding Neighbors For Customer ",i," / ",length(test.x))) # just a progress tracker.
}
dim(sim.df)
head(sim.df)
hist(sim.df$similarity)
###### Now making predictions about what our test_customers will buy in dependent period
test.customers <- as.character(unique(sim.df$test_customer))
make.neighbor.list <- function(x){
return(sim.df$train_customer[which(sim.df$test_customer == x)])
}
neighbor.list <- sapply(test.customers,make.neighbor.list)
neighbor.list <- lapply(neighbor.list,as.character)
make.predictions <- function(x){ # to be applied to neighbor.list
neighbors <- unlist(x)
pos <- which(names(train.y) %in% neighbors)
train.y.product.list <- train.y[pos]
x <- table(unlist(train.y.product.list)) / length(neighbors)
x <- x[order(x,decreasing=TRUE)]
return(x)
}
# A list of every single customer,and likely products they will buy in dependent period.
# The proportions represent the (%) of (k) neighbors who purchased those products
all.predictions <- lapply(neighbor.list,make.predictions)
# Now we need to evaluate these predictions
# Imputing all products not in neighbor pool with prediction scores of 0.
results.df <- data.frame(customer = NULL,prod = NULL,prediction=NULL)
products <- unique(base.table$ProductID)
# creating a data.frame of all predictions for all customers for all products.
for(i in 1:length(all.predictions)){
cust.names <- names(all.predictions)[i]
ex.df <- as.data.frame(all.predictions[i])
try(colnames(ex.df) <- c("product","score"))
try(pos <- which(names(test.y) == cust.names))
try(new.items <- as.vector(unlist(test.y[pos])))
try(all.preds <- rep(0,length(products)))
try(all.preds[which(products %in% ex.df$product)] <- ex.df$score)
try(customer <- rep(cust.names,nrow(test.results)))
try(test.results <- data.frame(customer,prod=products,prediction=all.preds))
try(results.df <- rbind(results.df,test.results))
print(paste0("Organizing Predictions for Customer: ",i," / ",length(all.predictions)))
}
return(results.df)
}
predictions <- predict.fx("2008-08-31")
rm(list=ls())
# FUNCTION TO CREATE BASETABLE
create_BT <- function(t1, t2, t3, t4,tr_data){
# INSTALL PACKAGES
if (!require("plyr")) install.packages('plyr'); library('plyr')
# LOADS DATA
hh_demo = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/hh_demographic.csv")
coup.red = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/coupon_redempt.csv")
#filtering customers
cust.pur = ddply(tr_data,.(household_key),summarise,
firstday = min(DAY), lastday = max(DAY))
cust.pur.filtered = cust.pur[which(cust.pur$firstday<t2 & cust.pur$lastday> t2-365),]
# Since household data is for only a few, adding a new level -> no data
c1 = merge(hh_demo,cust.pur.filtered, all = T)
fcts = sapply(c1, is.factor)
c1.1 = sapply(c1[,fcts], as.character)
c1.1[is.na(c1.1)] = "NO DATA"
c1.1 = sapply(c1.1, as.factor)
c1[,fcts] = factor(c1.1)
c1[sapply(c1, is.character)] <- lapply(c1[sapply(c1, is.character)], as.factor)
# number of coupons redeemed in independent period
coup.red = coup.red[which(coup.red$DAY<t2),]
coup.agg = ddply(coup.red, .(household_key), summarise, coup.no = length(COUPON_UPC))
c2 = merge(c1, coup.agg, all = T)
c2$coup.no[is.na(c2$coup.no)] = 0
tr_data.filtered = tr_data[which(tr_data$DAY<t2),]
tr_data.agg = ddply(tr_data.filtered,.(household_key),summarise,
no.basket = length(unique(BASKET_ID)),
no.product = length(unique(PRODUCT_ID)),
sum.quant = sum(QUANTITY),
tot.sales.value = sum(SALES_VALUE),
stores = length(unique(STORE_ID)),
retail.disc = sum(RETAIL_DISC),
coup.disc = sum(COUPON_DISC))
base = merge(tr_data.agg,c2)
# RETURN BASETABLE
return (base)
}
# FUNCTION TO BUILD MODE
model.build <- function(t1,t2,t3,t4,tr_data){
# CALL BASETABLE FUNCTION
base <- create_BT(t1, t2, t3, t4,tr_data)
length_ind <- t2 - t1
length_op <- t3 - t2
length_dep <- t4 - t3
## COMPUTE DV (1 if churn, else 0)
base$DV = as.factor(ifelse(base$lastday > t4,0,1))
# INSTALL PACKAGES
if (!require("randomForest")) {
install.packages('randomForest',
repos="https://cran.rstudio.com/",
quiet=TRUE)
require('randomForest')
}
if (!require("AUC")) {
install.packages('AUC',
repos="https://cran.rstudio.com/",
quiet=TRUE)
require('AUC')
}
if (!require("lift")) {
install.packages('lift',
repos="https://cran.rstudio.com/",
quiet=TRUE)
require('lift')
}
# RANDOMIZE INDICATORS
ind <- 1:nrow(base)
indTRAIN <- sample(ind,round(0.5*length(ind)))
indTEST <- ind[-indTRAIN]
# ISOLATE DV
DV <-base$DV
# DELETE COLUMNS
base$DV <- NULL
base$lastday <- NULL
# RUN MODEL
rFmodel <- randomForest(x=(base[indTRAIN,]),
y=DV[indTRAIN],
ntree=1000)
predrF <- predict(rFmodel,base[indTEST,],type="prob")[,2]
cat("AUC under ROC curve of the model:", AUC::auc(roc(predrF,DV[indTEST])))
cat("\nTop Decile Lift:", TopDecileLift(predrF,DV[indTEST]))
varImpPlot(rFmodel)
# RETURN MODEL, LENGTH OF INDEPENDENT, OPERATIONAL, DEPENDENT
return (list(rFmodel, length_ind, length_op, length_dep,predrF,DV[indTEST]))
}
# FUNCTION TO DEPLOY MODEL
model.predict <- function(object, dumpDate){
# TIMELINE
t2 <- dumpDate
t1 <- t2 - object[[2]]
t3 <- t2 + object[[3]]
t4 <- t3 + object[[4]]
# SAVE MODEL
rFmodel <- object[[1]]
# CREATE BASETABLE
predbase <- create_BT(t1, t2, t3, t4,tr_data)
predbase$lastday <- NULL
# RUN PREDICTIONS
predrF <- predict(rFmodel, predbase, type="prob")[,2]
# STORE OUTPUT (CUSTOMERID AND SCORE)
ans <- data.frame("Customer_Id" = predbase$household_key, "Score" = predrF)
ans <- ans[order(ans$Score, decreasing=TRUE),]
# RETURN ANSWER
ans
}
tr_data = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/transaction_data.csv")
tr_data = tr_data[which(tr_data$QUANTITY != 0 & tr_data$SALES_VALUE != 0),]
t1 = min(tr_data$DAY)
t4 = max(tr_data$DAY) - 90
t3 = t4 - 180
t2 = t3 - 1
# CALL MODEL.BUILD
Cmodel <- model.build(t1, t2, t3, t4,tr_data)
# CALL MODEL.PREDICT
pred <- model.predict(object=Cmodel, dumpDate=max(tr_data$DAY))
p = Cmodel[[5]]
c = Cmodel[[6]]
library(ROCR)
pr = prediction(p,c)
perf <- performance(pr, measure="prec", x.measure="cutoff")
plot(perf)
perf <- performance(pr, measure="rec", x.measure="cutoff")
plot(perf)
library(DMwR)
PRcurve(p,c)
#From the curves, it seems a threshold of around 0.35 is reasonable
table(p<0.35,c)
churn.hou = pred[which(pred$Score>0.35),]$Customer_Id
length(churn.hou)
library(recommenderlab)
library(ggplot2)
library(reshape2)
hh.tr.data = ddply(tr_data, .(PRODUCT_ID), summarize,tot.quantity = sum(QUANTITY))
prod.consider = unique(hh.tr.data[which(hh.tr.data$tot.quantity>20),]$PRODUCT_ID)
tr.data.filtered = tr_data[which(tr_data$PRODUCT_ID %in% prod.consider),]
rm(tr_data)
rm(hh.tr.data)
hh.tr.data = ddply(tr.data.filtered, .(household_key,PRODUCT_ID), summarize,tot.quantity =log(length(QUANTITY)))
g = acast(hh.tr.data, household_key~PRODUCT_ID)
r <- as(g, "realRatingMatrix")
#Evaluation of different schemes
qplot(getRatings(r), binwidth = 0.5,
main = "Histogram of log of quantities", xlab = "log Qty") # Skewed to the left
qplot(getRatings(normalize(r, method = "Z-score")),
main = "Histogram of log of quantities", xlab = "log Qty") # better
#since log(2) = 0.69, we're saying goodRating = 0.7, meaning they will buy it 2 times
scheme <- evaluationScheme(r, method = "cross-validation", train = .9,
k = 10, given = -1, goodRating = 0.7)
scheme
# Not looking at IBCF, since that won't help our cause here
algorithms <- list(
"random items"  = list(name="RANDOM", param=list(normalize = "Z-score")),
"popular items" = list(name="POPULAR", param=list(normalize = "Z-score")),
"user-based CF" = list(name="UBCF", param=list(normalize = "Z-score",
method="Cosine",
nn=20))
)
# run algorithms, predict next n items
results <- evaluate(scheme, algorithms, n=c(1, 3, 5, 10, 15))
# Draw ROC curve
plot(results, annotate = 1:4, legend="topleft")
# See precision / recall
plot(results, "prec/rec", annotate=3)
?cite
Rec.model=Recommender(r,method="UBCF",
param=list(normalize = "Z-score",method="Cosine",nn=20))
pr = predict(Rec.model, r[churn.hou,], n=3)
r
qplot(rowCounts(r), binwidth = 10,
main = "Products bought on average",
xlab = "# of users",
ylab = "# of products purchased")
as(pr, "list")
prod = read.csv("/home/tapajit/Courses/fall 2016/bzan552/dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/product.csv")
View(prod)
cite(Recommender())
?cite
cite("R")
citation()
citation("RandomForest")
citation("randomForest")
citation("lift")
citation("AUC")
citation("plyr")
citation("recommenderlab")
system("python Desktop/s.py")
?rnorm
?biplot
setwd("~/Work/release_qual/model/images")
#library
library(plyr)
library(GGally)
library(ggplot2)
library(psych)
library(DataCombine)
library(reshape)
library(gridExtra)
library(grid)
#Reading .user data
filelist <- list.files(path = "../../data/data1_May16/", pattern = "*4.new.user", full.names = TRUE)
#Reading New data
filelist1 <- list.files(path = "../../data/data1_May16/", pattern = "*4.new$", full.names = TRUE)
UserFile = do.call(rbind, lapply(filelist, function(x) read.csv(file = x,sep=";",na.strings="(not set)")))
UserFile[,7] <- as.Date(UserFile[,7],"%Y%m%d")
UserFile[,8] <- as.integer(UserFile[,8])
UserFile[,9] <- as.integer(UserFile[,9])
UserFile[,10] <- as.numeric(UserFile[,10])
UserFile <- UserFile[order(UserFile[,7]),]
NewData <- do.call(rbind, lapply(filelist1, function(x) read.csv(file = x,sep=";")))
NewData[,3] <- as.Date(as.character(NewData[,3]),"%Y%m%d")
NewData <- NewData[order(NewData[,3]),]
total = merge(NewData,UserFile,all=T)
total = total[complete.cases(total),]
total$ga.deviceCategory <- factor(total$ga.deviceCategory)
total$ga.fatalExceptions <- NULL
total$ga.timeOnSite  <- NULL
###############################################################################################################
###############################################################################################################
library(plyr)
library(GGally)
library(ggplot2)
library(psych)
#2.0.0_317 last day,2.0.0_326 first 2 days,2.0.0_350 last day, 435 last day, 2.1.0_483 last 2, 503 first, 3.0.0_197 last
total = total[!rownames(total) %in% c(504,759,761,2010,12248,13245,13244,13825,49530,48963),]
#collapsing all releases to get uniform curve
releases = unique(total$ga.appVersion)
goodrelease = c()
rd = matrix(nrow = 0, ncol=10)
for (r in sort(releases)){
y= total[total$ga.appVersion == r,]
z= ddply(y, .(ga.date), summarise,
nu = sum(ga.newUsers),
nv = sum(ga.newVisits),
tu = sum(ga.users),
tv = sum(ga.visits),
ex = sum(ga.exceptions))
#removing suspicious releases
if (quantile(z$nu,0.1) == quantile(z$nu,0.9) || quantile(z$nv,0.1) == quantile(z$nv,0.9)) next()
#keeping only releases with >7 days of data or non zero number of exceptions
if(nrow(z) > 7){
z$cnu = cumsum(z$nu)
quantile(z$cnu)
goodrelease = c(goodrelease,r)
png(paste0(r,".png"),width = 960, height = 960)
pairs.panels(z)
dev.off()
rd = rbind(rd,(c(r, min(z$ga.date), max(z$ga.date),(z[which(z$cnu > quantile(z$cnu,0.5)),]$ga.date)[1],
(z[which(z$cnu > quantile(z$cnu,0.25)),]$ga.date)[1],(z[which(z$cnu > quantile(z$cnu,0.75)),]$ga.date)[1],
(z[which(z$cnu >= quantile(z$cnu,0.9)),]$ga.date)[1],sum(z$nu),sum(z$ex),sum(z$nv))))
}
}
rd = as.data.frame(rd)
colnames(rd) = c("Release","Mindate","Maxdate","Median","25.Quant","75.Quant","90.Quant","Total.NU","ex","Total.NV")
rd$Total.NU = as.numeric(as.character(rd$Total.NU))
Total.NU = as.numeric(as.character(rd$Total.NU))
rd$Total.NV = as.numeric(as.character(rd$Total.NV))
ex = as.numeric(as.character(rd$ex))
rd$ex = ex
rd$Mindate = as.Date(as.integer(as.character(rd$Mindate)),origin="1970-01-01")
rd$Maxdate = as.Date(as.integer(as.character(rd$Maxdate)),origin="1970-01-01")
rd$Median = as.Date(as.integer(as.character(rd$Median)),origin="1970-01-01")
rd$`25.Quant` = as.Date(as.integer(as.character(rd$`25.Quant`)),origin="1970-01-01")
rd$`75.Quant` = as.Date(as.integer(as.character(rd$`75.Quant`)),origin="1970-01-01")
rd$`90.Quant` = as.Date(as.integer(as.character(rd$`90.Quant`)),origin="1970-01-01")
rd$Release = as.character(rd$Release)
refr = rd
tick.col = c()
for (i in 1:nrow(rd)){
if (ecdf(Total.NU)(rd[i,8]) <0.2){
rd[i,1] = paste0("",rd[i,1])
} else if (ecdf(Total.NU)(rd[i,8]) <0.5){
rd[i,1] = paste0("*",rd[i,1])
}else if (ecdf(Total.NU)(rd[i,8]) <0.8){
rd[i,1] = paste0("***",rd[i,1])
}else {
rd[i,1] = paste0("******",rd[i,1])
}
if (rd[i,9] == 0) {
tick.col = c(tick.col,"green")
} else if (rd[i,9] < 50){
tick.col = c(tick.col,"black")
} else if (rd[i,9] < 300){
tick.col = c(tick.col,"blue")
} else if (rd[i,9] < 750){
tick.col = c(tick.col,"orange")
}else {
tick.col = c(tick.col,"red")
}
}
rd$`25.Quant` = NULL
rd$`75.Quant` = NULL
rd$ex = NULL
rd$Total.NU = NULL
rd$Total.NV = NULL
# release dates added manually
manrd = data.frame("Release"=as.factor(c("******2.1.2_568","***2.1.2_574","******2.1.4_577","******2.0.0_340","******2.0.1_372","******2.1.0_527")),
"Release Date"=as.Date(c("2015-10-02","2016-01-06","2016-01-22","2014-03-17","2014-05-20","2014-12-15")))
nrd = merge(rd,manrd,all = T)
r=melt(nrd, id="Release")
ggplot(r) + geom_point(aes(x=Release, y=value, colour=variable, shape=variable), size=7) +
scale_color_manual(values = c("red","blue","green","magenta","black")) + scale_shape_manual(values = c(18,16,17,15,8))+
ylab("Year")+
theme_bw(base_size = 20) +
theme(axis.text.y = element_text(colour=tick.col, size=20), axis.title = element_text(size = 20),
axis.text.x = element_text(colour="black", size=20)) +
coord_flip()
library(bnlearn)
refr$Release = as.factor(refr$Release)
refr$duration = as.numeric(refr$`90.Quant` - refr$Mindate)
refr$f.duration = as.numeric(refr$Maxdate - refr$Mindate)
refr[,3:7] = NULL
refr$Release = NULL
refr$VperU = refr$Total.NV / refr$Total.NU
refr$durFromLast = rep(0, nrow(refr))
for (i in 2:nrow(refr)){
refr[i,8] = refr[i,1] - refr[(i-1),1]
}
refr2 = refr
refr2$Mindate = as.numeric(refr2$Mindate)
refr = refr2
refr$Total.NU = log(refr$Total.NU+1)
refr$duration = log(refr$duration+1)
refr$ex = log(refr$ex + 1)
refr$VperU = log(refr$VperU)
refr$durFromLast = log(abs(refr$durFromLast)+1)
tnv = refr$Total.NV
refr$Total.NV = NULL
refr$f.duration = NULL
refr$Mindate = NULL
refr = data.frame(sapply(refr,function(x) log(x+1)))
refrs = data.frame(sapply(refr,scale))
View(total)
View(refr)
View(refr2)
pdag1 = iamb(refr)
plot(pdag1)
dag1 = cextend(pdag1)
plot(dag1)
(b1 = bn.fit(dag1,refrs))
cite()
cite(R)
cite("R")
?cite
citation()
citation("bnlearn")
citation(plyr)
citation("plyr")
citation
citation("GGally")
citation("ggplot2")
citation("pych")
citation("psych")
citation("DataCombine")
citation("reshape")
citation("deal")
citation("Rgraphviz")
citation(catnet)
citation("catnet")
citation("pcalg")
dag2 = tabu(refrs)
plot(dag2)
(b2 = bn.fit(dag2, refrs))
(var2 = b2$Total.NU$sd**2 + b2$ex$sd**2 + b2$duration$sd**2 + b2$VperU$sd**2 + b2$durFromLast$sd**2)
dag3 = rsmax2(refrs, restrict = "si.hiton.pc", maximize = "tabu",
test = "zf", alpha = 0.1, score = "bic-g")
plot(dag3)
